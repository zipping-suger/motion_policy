{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import h5py\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "def downsize_and_split(\n",
    "    input_file: str, output_dir: str, train_size: int, val_size: int, test_size: int\n",
    "):\n",
    "    \"\"\"\n",
    "    This function is meant to be used to regularize the sizes of individual problem types\n",
    "    (e.g. merged cubbies without neutral poses). Use this function on the output of either\n",
    "    `merge_data_pipeline_files` or `extract_hybrid_expert_data` (depending on whether you\n",
    "    want all the problems with global expert solutions or the subset that have hybrid expert solutions\n",
    "    as well). This function will create three datasets, a train, val, and test dataset.\n",
    "    If any of the sizes are set to 0, it will ignore that dataset\n",
    "\n",
    "    :param input_file str: The input file, should come from one of the functions described above\n",
    "    :param output_dir str: The output directory (this directory should exist but doesn't\n",
    "                           need any subdirectories)\n",
    "    :param train_size int: The size of the training dataset\n",
    "    :param val_size int: The size of the validation dataset\n",
    "    :param test_size int: The size of the test dataset\n",
    "    \"\"\"\n",
    "    with h5py.File(input_file) as f:\n",
    "        assert train_size + val_size + test_size < len(f[\"cuboid_centers\"])\n",
    "        indices = np.random.choice(\n",
    "            np.arange(len(f[\"cuboid_centers\"])),\n",
    "            size=train_size + test_size + val_size,\n",
    "            replace=False,\n",
    "        )\n",
    "        train_indices, val_indices, test_indices = (\n",
    "            np.sort(indices[:train_size]),\n",
    "            np.sort(indices[train_size : train_size + val_size]),\n",
    "            np.sort(indices[train_size + val_size :]),\n",
    "        )\n",
    "\n",
    "        assert (\n",
    "            len(train_indices) + len(val_indices) + len(test_indices)\n",
    "            == train_size + val_size + test_size\n",
    "        )\n",
    "\n",
    "        path = Path(output_dir).resolve()\n",
    "\n",
    "        if val_size > 0:\n",
    "            (path / \"val\").mkdir(parents=True, exist_ok=True)\n",
    "            with h5py.File(str(path / \"val\" / \"val.hdf5\"), \"w-\") as g:\n",
    "                for k in f.keys():\n",
    "                    g.create_dataset(k, (val_size, *f[k].shape[1:]))\n",
    "                for ii, jj in enumerate(tqdm(val_indices)):\n",
    "                    for k in g.keys():\n",
    "                        g[k][ii, ...] = f[k][jj, ...]\n",
    "        if test_size > 0:\n",
    "            (path / \"test\").mkdir(parents=True, exist_ok=True)\n",
    "            with h5py.File(str(path / \"test\" / \"test.hdf5\"), \"w-\") as g:\n",
    "                for k in f.keys():\n",
    "                    g.create_dataset(k, (test_size, *f[k].shape[1:]))\n",
    "                for ii, jj in enumerate(tqdm(test_indices)):\n",
    "                    for k in g.keys():\n",
    "                        g[k][ii, ...] = f[k][jj, ...]\n",
    "        if train_size > 0:\n",
    "            (path / \"train\").mkdir(parents=True, exist_ok=True)\n",
    "            with h5py.File(str(path / \"train\" / \"train.hdf5\"), \"w-\") as g:\n",
    "                for k in f.keys():\n",
    "                    g.create_dataset(k, (train_size, *f[k].shape[1:]))\n",
    "                for ii, jj in enumerate(tqdm(train_indices)):\n",
    "                    for k in g.keys():\n",
    "                        g[k][ii, ...] = f[k][jj, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the dataset: 10552\n",
      "Keys in the dataset: ['cuboid_centers', 'cuboid_dims', 'cuboid_quaternions', 'cylinder_centers', 'cylinder_heights', 'cylinder_quaternions', 'cylinder_radii', 'global_solutions']\n",
      "Trajectory length of global_solutions: 50\n"
     ]
    }
   ],
   "source": [
    "input_file=\"../data/cubby/neutral/curobo/all_data.hdf5\"\n",
    "with h5py.File(input_file) as f:\n",
    "    # Get the number of samples in the dataset\n",
    "    num_samples = len(f[\"cuboid_centers\"])\n",
    "    print(f\"Number of samples in the dataset: {num_samples}\")\n",
    "    \n",
    "    # Get the keys in the dataset\n",
    "    dataset_keys = list(f.keys())\n",
    "    print(f\"Keys in the dataset: {dataset_keys}\")\n",
    "    \n",
    "    \n",
    "    # check global_solutions trajectory length\n",
    "    global_solutions = f[\"global_solutions\"]\n",
    "    trajectory_length = global_solutions.shape[1]\n",
    "    print(f\"Trajectory length of global_solutions: {trajectory_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22cb0c4bffc4103a11f1af675403083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f521bbf906e4a6fa94fb4db9badadb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "downsize_and_split(input_file=\"../data/cubby/neutral/curobo/all_data.hdf5\", \n",
    "                   output_dir=\"pretrain_data\", \n",
    "                   train_size=6000, \n",
    "                   val_size=1000, \n",
    "                   test_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases found: [PosixPath('pretrain_data/train/train.hdf5')]\n",
      "Databases found: [PosixPath('pretrain_data/val/val.hdf5')]\n"
     ]
    }
   ],
   "source": [
    "# Test loading the data\n",
    "from data_loader import DataModule\n",
    "\n",
    "# Define parameters for the DataModule\n",
    "data_dir = \"pretrain_data\"  # Replace with the actual path to your data\n",
    "trajectory_key = \"global_solutions\"  # Replace with the actual key in your dataset\n",
    "num_robot_points = 2048\n",
    "num_obstacle_points = 4096\n",
    "random_scale = 0  # For MpiNet, it's 0.015 \n",
    "batch_size = 32\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_module = DataModule(\n",
    "    data_dir=data_dir,\n",
    "    trajectory_key=trajectory_key,\n",
    "    num_robot_points=num_robot_points,\n",
    "    num_obstacle_points=num_obstacle_points,\n",
    "    random_scale=random_scale,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# Setup the DataModule\n",
    "data_module.setup(stage=\"fit\")\n",
    "\n",
    "# Access the dataloaders\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train loader: 7813\n",
      "Number of batches in validation loader: 63\n",
      "No NaN found in validation data.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Check the number of batches in the train and validation loaders\n",
    "print(f\"Number of batches in train loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in validation loader: {len(val_loader)}\")\n",
    "\n",
    "\n",
    "# Check is there is NaN in xyz of Validation data\n",
    "for batch in val_loader:\n",
    "    # Check if there are NaN values in the batch\n",
    "    if torch.isnan(batch[\"xyz\"]).any():\n",
    "        print(\"NaN found in validation data!\")\n",
    "        break\n",
    "else:\n",
    "    print(\"No NaN found in validation data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pointnet2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
