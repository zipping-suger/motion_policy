train_mode: "finetune"  # "pretrain" or "fine-tune"

training_model_parameters:
    pc_latent_dim: 2048
    goal_loss_weight: 1
    collision_loss_weight: 10
    action_scale: 0.1  # Scale for the action space

data_module_parameters:
  data_dir: "./pretrain_data/single_cubby_17k"
  trajectory_key: 'global_solutions' # Could also use 'global_solutions' for the global expert
  num_obstacle_points: 4096
  num_target_points: 128
  random_scale: 0

shared_parameters:
    num_robot_points: 2048

model_path: "./checkpoints/cubby_pretrain/last.ckpt"
checkpoint_interval: 10
validation_interval: 10
accelerator: "gpu"
gpu_num: 1  # If > 1, PyTorch Lightning will automatically use DDP for training
batch_size: 128
save_checkpoint_dir: "./checkpoints"
experiment_name: reach_training_single_free8k_scratch_b128_opt
